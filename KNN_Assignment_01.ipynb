{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-01`    What is the KNN algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `K-Nearest Neighbors` (KNN) algorithm is a simple yet effective supervised machine learning algorithm used for classification and regression tasks. It belongs to the family of instance-based, lazy learning algorithms because it doesn't explicitly learn a model during training. Instead, it memorizes the entire training dataset and makes predictions based on the similarity of new instances to known instances in the training data.**\n",
    "\n",
    "**`Here's how the KNN algorithm works` :**\n",
    "\n",
    "1. **Initialization -** KNN requires a predefined value for 'K', which represents the number of nearest neighbors to consider when making predictions.\n",
    "\n",
    "2. **Calculate distances -** When given a new, unseen instance, KNN calculates the distance between this instance and every other instance in the training dataset. The distance metric commonly used is Euclidean distance, although other metrics like Manhattan distance or cosine similarity can also be used depending on the problem.\n",
    "\n",
    "3. **Find nearest neighbors -** Once distances are calculated, the algorithm selects the 'K' instances (neighbors) with the smallest distances to the new instance.\n",
    "\n",
    "4. **Majority voting (for classification) or averaging (for regression) -** For classification tasks, KNN predicts the class of the new instance by taking a majority vote among the classes of its nearest neighbors. For regression tasks, it predicts the average value of the target variable among its nearest neighbors.\n",
    "\n",
    "5. **Prediction -** After determining the class (for classification) or value (for regression), the algorithm assigns this as the prediction for the new instance.\n",
    "\n",
    "**`KNN is a non-parametric algorithm`, meaning it doesn't make any assumptions about the underlying data distribution.** It is also versatile and can be applied to various types of data. However, KNN can be computationally expensive, especially with large datasets, as it requires calculating distances to all instances in the training data for each prediction. Additionally, choosing an appropriate value for 'K' can significantly impact the algorithm's performance, and the choice often depends on the specific dataset and problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-02`    How do you choose the value of K in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the value of $ k $ in the k-Nearest Neighbors (KNN) algorithm is a critical step as it significantly impacts the model's performance.**\n",
    "\n",
    "**`Here are some common methods for selecting the optimal value of`** $ k $ **:**\n",
    "\n",
    "1. **Cross-Validation -** Divide your dataset into $ k $ subsets (folds). Train the model on $ k-1 $ folds and validate it on the remaining fold. Repeat this process $ k $ times, each time with a different fold held out for validation. Compute the accuracy (or other relevant metric) for each $ k $ value, and choose the $ k $ that gives the best performance.\n",
    "\n",
    "2. **Grid Search -** Perform a grid search over a range of possible $ k $ values. Train and evaluate the model using each value of $ k $, and choose the one that yields the best performance.\n",
    "\n",
    "3. **Rule of Thumb -** A common rule of thumb is to take $ \\sqrt{n} $ or $ \\sqrt[3]{n} $, where $ n $ is the number of samples in the training dataset. This is a rough heuristic that often works well in practice but should be validated with cross-validation or other methods.\n",
    "\n",
    "4. **Domain Knowledge -** Depending on the specific problem and dataset, you may have domain knowledge that suggests a reasonable range for $ k $. For example, if you know that the decision boundaries are smooth, choosing a larger $ k $ might be appropriate.\n",
    "\n",
    "5. **Experimentation -** Sometimes, there's no substitute for experimentation. Try different values of $ k $ and evaluate their performance on a validation set or through cross-validation. Choose the value that gives the best results.\n",
    "\n",
    "6. **Bias-Variance Tradeoff -** Consider the bias-variance tradeoff. Smaller values of $ k $ tend to have low bias but high variance, leading to overfitting. Larger values of $ k $ have higher bias but lower variance. Choose $ k $ that balances this tradeoff effectively for your dataset.\n",
    "\n",
    "7. **Plotting Validation Curve -** Plot the validation accuracy or error against different $ k $ values. This can provide visual insight into how the model's performance changes with different $ k $ values, helping you choose the most appropriate one.\n",
    "\n",
    "It's essential to remember that the optimal value of $ k $ may vary depending on the dataset and the problem you're trying to solve. Therefore, it's crucial to experiment with different values and select the one that generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-03`    What is the difference between KNN classifier and KNN regressor?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors (KNN) is a simple yet powerful algorithm used for both classification and regression tasks. `The main difference between KNN classifier and KNN regressor lies in the nature of the task they are designed to solve` :**\n",
    "\n",
    "1. **KNN Classifier -**\n",
    "\n",
    "   - KNN classifier is used for classification tasks where the output variable is categorical.\n",
    "\n",
    "   - The algorithm classifies data points based on the majority class among their k-nearest neighbors.\n",
    "\n",
    "   - It assigns the class label that appears most frequently among the k-nearest neighbors of the data point being classified.\n",
    "\n",
    "   - The decision boundary in KNN classification is typically non-linear and is determined by the distribution of the training data in feature space.\n",
    "\n",
    "2. **KNN Regressor -**\n",
    "\n",
    "   - KNN regressor, on the other hand, is used for regression tasks where the output variable is continuous.\n",
    "\n",
    "   - Instead of predicting a class label, KNN regressor predicts a continuous value for each data point based on the average (or weighted \n",
    "   average) of the target values of its k-nearest neighbors.\n",
    "\n",
    "   - The predicted value for a given data point is typically the mean (or median) of the target values of its k-nearest neighbors.\n",
    "\n",
    "   - In KNN regression, the decision boundary is determined by the distribution of the training data in feature space, and the predicted value for a query point is influenced by the values of its nearest neighbors.\n",
    "\n",
    "`In summary`, the primary distinction between KNN classifier and KNN regressor lies in the type of output they produce: KNN classifier predicts class labels for categorical data, while KNN regressor predicts continuous values for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-04`    How do you measure the performance of KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The performance of a K-Nearest Neighbors (KNN) algorithm can be measured using various evaluation metrics depending on the specific task at hand (e.g., classification or regression).**\n",
    "\n",
    "**`Here are some common methods to measure the performance of a KNN algorithm` :**\n",
    "\n",
    "1. **Classification Accuracy -** For classification tasks, accuracy is a common metric. It measures the proportion of correctly classified instances out of the total instances. It's calculated as :\n",
    "\n",
    "   $$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$\n",
    "\n",
    "2. **Confusion Matrix -** A confusion matrix provides a more detailed breakdown of correct and incorrect classifications for each class. From the confusion matrix, you can derive various performance metrics like precision, recall, and F1-score.\n",
    "\n",
    "3. **Precision -** Precision measures the proportion of true positive predictions out of all positive predictions made by the classifier. It's calculated as :\n",
    "\n",
    "   $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$\n",
    "\n",
    "4. **Recall (Sensitivity) -** Recall measures the proportion of true positive predictions out of all actual positive instances in the dataset. It's calculated as :\n",
    "\n",
    "   $$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\n",
    "\n",
    "5. **F1-score -** F1-score is the harmonic mean of precision and recall. It provides a balanced measure between precision and recall. It's calculated as :\n",
    "\n",
    "   $$ \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "6. **Receiver Operating Characteristic (ROC) Curve -** For binary classification problems, the ROC curve plots the true positive rate against the false positive rate at various threshold settings. The area under the ROC curve (AUC-ROC) is a commonly used metric to evaluate classifier performance.\n",
    "\n",
    "7. **Mean Absolute Error (MAE) -** For regression tasks, MAE is a common metric. It measures the average absolute difference between predicted values and true values.\n",
    "\n",
    "8. **Mean Squared Error (MSE) -** Another regression metric, MSE measures the average of the squares of the errors between predicted values and true values.\n",
    "\n",
    "9. **R-squared (Coefficient of Determination) -** R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates perfect predictions.\n",
    "\n",
    "`When evaluating the performance of a KNN algorithm`, it's essential to consider the specific characteristics of the dataset and the requirements of the task to choose the most appropriate evaluation metrics. Additionally, cross-validation techniques can be employed to ensure the robustness of performance evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-05`    What is the curse of dimensionality in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The curse of dimensionality refers to the phenomenon where the performance of certain algorithms, including the k-nearest neighbors (KNN) algorithm, deteriorates significantly as the number of dimensions or features in the dataset increases.**\n",
    "\n",
    "**`In the context of KNN, the curse of dimensionality manifests in several ways` :**\n",
    "\n",
    "1. **Increased computational complexity -** As the number of dimensions increases, the computational cost of finding the nearest neighbors grows exponentially. This is because calculating distances in high-dimensional spaces becomes more computationally intensive.\n",
    "\n",
    "2. **Sparse data distribution -** In high-dimensional spaces, data points tend to become more sparse, meaning that the distance between nearest neighbors increases. Consequently, it becomes more challenging for KNN to accurately identify the nearest neighbors, leading to degraded performance.\n",
    "\n",
    "3. **Diminished discrimination between points -** In higher dimensions, the relative differences in distances between points become less meaningful. This can result in similar distances between all points, making it difficult for KNN to distinguish between classes or clusters effectively.\n",
    "\n",
    "4. **Increased risk of overfitting -** With a large number of dimensions, KNN may become more prone to overfitting, particularly when the number of neighbors (k) is small. This is because the model may capture noise or irrelevant features present in high-dimensional spaces.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN, various techniques can be employed, including dimensionality reduction methods such as Principal Component Analysis (PCA), feature selection techniques, or using distance metrics that are less sensitive to high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-06`    How do you handle missing values in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling missing values in KNN (K-Nearest Neighbors) involves various strategies to impute or deal with the absence of values in the dataset.**\n",
    "\n",
    "**`Here are some common approaches` :**\n",
    "\n",
    "1. **Ignore missing values -** One simple approach is to ignore instances with missing values during the computation of distances. However, this might lead to a loss of valuable information, especially if a significant portion of the data has missing values.\n",
    "\n",
    "2. **Mean/Median/Mode imputation -** Replace missing values with the mean, median, or mode of the feature across the dataset. This method can work well for numerical data but may not be suitable for categorical features.\n",
    "\n",
    "3. **KNN Imputation -** Use KNN to impute missing values by considering the K nearest neighbors of the instance with missing values. The missing value is replaced by the average (for numerical features) or mode (for categorical features) of the corresponding feature among its nearest neighbors.\n",
    "\n",
    "4. **Interpolation -** Perform interpolation to estimate missing values based on the values of neighboring instances. Techniques like linear interpolation or spline interpolation can be used, especially for time-series data.\n",
    "\n",
    "5. **Predictive models -** Train predictive models (e.g., regression models or decision trees) to predict missing values based on other features in the dataset. Once trained, these models can be used to impute missing values.\n",
    "\n",
    "6. **Feature scaling -** Ensure that features are scaled appropriately before applying KNN. This is because KNN relies on distance measures, and features with different scales may have disproportionate influences on the distance calculations.\n",
    "\n",
    "7. **Weighted KNN -** Instead of considering the same weight for all neighbors, assign weights to each neighbor based on their distance. Closer neighbors may have higher weights, thereby influencing the imputation process more.\n",
    "\n",
    "8. **Multiple Imputation -** Perform multiple imputations by applying one of the above methods multiple times and averaging the results. This can help in capturing the uncertainty associated with imputed values.\n",
    "\n",
    "The choice of method depends on various factors such as the nature of the data, the extent of missing values, and the computational resources available. It's also essential to evaluate the performance of the chosen method using appropriate validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-07`    Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors (KNN) is a versatile algorithm used for both classification and regression tasks. `Let's compare and contrast the performance of KNN classifier and regressor` :**\n",
    "\n",
    "1. **KNN Classifier -**\n",
    "\n",
    "   - **Task**: KNN classifier is used for classification tasks where the output is a categorical label.\n",
    "\n",
    "   - **Prediction**: It predicts the class of a data point based on the classes of its nearest neighbors.\n",
    "\n",
    "   - **Output**: The output of KNN classifier is a class label.\n",
    "\n",
    "   - **Distance Metric**: Typically, it uses distance metrics like Euclidean distance to measure the similarity between data points.\n",
    "\n",
    "   - **Decision Boundary**: The decision boundary is non-linear and can adapt to the underlying data distribution.\n",
    "\n",
    "   - **Performance**: KNN classifier tends to perform well when the decision boundaries are irregular or complex, and when there are no clear separations between classes.\n",
    "\n",
    "   - **Parameters**: Important parameters include the number of neighbors (K) and the choice of distance metric.\n",
    "\n",
    "2. **KNN Regressor -**\n",
    "\n",
    "   - **Task**: KNN regressor is used for regression tasks where the output is a continuous value.\n",
    "\n",
    "   - **Prediction**: It predicts the value of a data point based on the average (or weighted average) of the values of its nearest neighbors.\n",
    "\n",
    "   - **Output**: The output of KNN regressor is a continuous value.\n",
    "\n",
    "   - **Distance Metric**: Similar to KNN classifier, it often uses distance metrics like Euclidean distance.\n",
    "\n",
    "   - **Decision Boundary**: There is no decision boundary concept in regression. Instead, KNN regressor estimates the target value based on the proximity of neighboring data points.\n",
    "\n",
    "   - **Performance**: KNN regressor tends to perform well when the data has a smooth and continuous underlying structure. However, it may struggle when the relationships between features and target variables are complex or non-linear.\n",
    "\n",
    "   - **Parameters**: Similar to KNN classifier, the number of neighbors (K) is an important parameter. Additionally, the weighting scheme (uniform or distance-based weights) can also impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which one is better for which type of problem?**\n",
    "\n",
    "- **KNN Classifier -** It is suitable for classification tasks where the decision boundaries are complex or non-linear, and when the data is not well-separated into distinct classes. It can handle multi-class classification problems effectively.\n",
    "  \n",
    "- **KNN Regressor -** It is suitable for regression tasks where the relationships between features and target variables are relatively smooth and continuous. It can be effective for tasks such as predicting house prices, stock prices, or any other continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In summary`, the choice between KNN classifier and regressor depends on the nature of the problem and the characteristics of the dataset. If the target variable is categorical, KNN classifier is more appropriate, while for continuous target variables, KNN regressor is the preferred choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-08`    What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The K-Nearest Neighbors (KNN) algorithm is a popular choice for both classification and regression tasks due to its simplicity and effectiveness. However, it also has its own set of strengths and weaknesses.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Strengths` :**\n",
    "\n",
    "1. **Simplicity -** KNN is straightforward and easy to understand, making it a good starting point for beginners in machine learning.\n",
    "  \n",
    "2. **No Training Phase -** Unlike many other algorithms, KNN does not require a training phase. It memorizes the training data instead, which makes the training process very fast.\n",
    "\n",
    "3. **Non-parametric -** KNN makes no assumptions about the underlying data distribution, making it suitable for a wide range of problems, including those where the decision boundary is highly irregular.\n",
    "\n",
    "4. **Adaptability -** KNN can be adapted to different types of data and distances by choosing an appropriate distance metric and value for k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Weaknesses` :**\n",
    "\n",
    "1. **Computational Complexity -** As the size of the dataset increases, the computational cost of KNN also increases because it requires computing distances to all training samples for each prediction.\n",
    "\n",
    "2. **Memory Consumption -** Since KNN stores all the training data, it can be memory-intensive, especially for large datasets.\n",
    "\n",
    "3. **Sensitive to Noise and Irrelevant Features -** KNN can perform poorly in the presence of noisy data or irrelevant features because it considers all features equally without any feature weighting.\n",
    "\n",
    "4. **Need for Optimal K Value -** The performance of KNN can be sensitive to the choice of the number of neighbors (k). Choosing an inappropriate value for k can lead to overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Addressing Weaknesses` :**\n",
    "\n",
    "1. **Dimensionality Reduction -** If dealing with high-dimensional data, dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection methods can be applied to reduce the computational cost and address the curse of dimensionality.\n",
    "\n",
    "2. **Distance Metrics -** Choosing an appropriate distance metric can significantly impact the performance of KNN. Experimenting with different distance metrics such as Euclidean, Manhattan, or Minkowski distances can help improve results.\n",
    "\n",
    "3. **Feature Engineering -** Removing irrelevant features and reducing noise in the dataset through feature engineering techniques can improve the performance of KNN.\n",
    "\n",
    "4. **Cross-Validation for k Selection -** Performing cross-validation to select the optimal value of k can help prevent overfitting or underfitting and improve generalization performance.\n",
    "\n",
    "5. **Algorithmic Improvements -** Various algorithmic improvements like KD-trees or Ball trees can be employed to speed up the search for nearest neighbors, reducing computational complexity.\n",
    "\n",
    "6. **Ensemble Methods -** Combining multiple KNN models or using ensemble methods like Bagging or Boosting can help improve the robustness and generalization performance of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`By addressing these weaknesses`, the performance of the KNN algorithm can be improved, making it more suitable for a wider range of classification and regression tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-09`    What is the difference between Euclidean distance and Manhattan distance in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euclidean distance and Manhattan distance are both distance metrics used in K-Nearest Neighbors (KNN) algorithm to measure the similarity between data points. However, they differ in how they calculate distance.**\n",
    "\n",
    "1. **Euclidean Distance -**\n",
    "\n",
    "   - Euclidean distance is the straight-line distance between two points in a Euclidean space. \n",
    "\n",
    "   - Mathematically, the Euclidean distance between two points $(x_1, y_1)$ and $(x_2, y_2)$ in a 2-dimensional space is calculated using the formula :\n",
    "     $$ \\text{Euclidean distance} = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$\n",
    "\n",
    "   - In higher dimensions, the formula extends to :\n",
    "     $$ \\text{Euclidean distance} = \\sqrt{\\sum_{i=1}^{n} (x_{2i} - x_{1i})^2} $$\n",
    "\n",
    "   - Euclidean distance is sensitive to the magnitude of differences. It's the most commonly used distance metric and is suitable for continuous data.\n",
    "\n",
    "2. **Manhattan Distance -**\n",
    "\n",
    "   - Manhattan distance, also known as city block distance or taxicab distance, calculates the distance between two points by summing the absolute differences between their coordinates.\n",
    "\n",
    "   - Mathematically, the Manhattan distance between two points $(x_1, y_1)$ and $(x_2, y_2)$ in a 2-dimensional space is calculated using the formula :\n",
    "     $$ \\text{Manhattan distance} = |x_2 - x_1| + |y_2 - y_1| $$\n",
    "\n",
    "   - In higher dimensions, the formula extends to:\n",
    "     $$ \\text{Manhattan distance} = \\sum_{i=1}^{n} |x_{2i} - x_{1i}| $$\n",
    "\n",
    "   - Manhattan distance is less sensitive to outliers compared to Euclidean distance and is particularly useful when dealing with data with a high dimensionality or when feature scales are not consistent.\n",
    "\n",
    "`In summary`, the key differences between Euclidean distance and Manhattan distance lie in their calculation methods and sensitivity to feature scales. Euclidean distance calculates the straight-line distance, while Manhattan distance calculates distance by summing the absolute differences between coordinates. The choice between these distance metrics often depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-10`    What is the role of feature scaling in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling is an important preprocessing step in many machine learning algorithms, including k-Nearest Neighbors (KNN). `The role of feature scaling in KNN is to ensure that all features contribute equally to the distance calculations between data points`.**\n",
    "\n",
    "`In KNN`, the algorithm classifies a new data point by comparing it to the k nearest data points in the training set based on a distance metric, typically Euclidean distance. If features are on different scales, those with larger magnitudes can dominate the distance calculations and overshadow the contributions of features with smaller magnitudes.\n",
    "\n",
    "By scaling the features to a similar range, feature scaling prevents this dominance issue and ensures that all features have equal weight in determining distances. Common techniques for feature scaling include normalization (scaling features to a range between 0 and 1) and standardization (scaling features to have mean 0 and standard deviation 1).\n",
    "\n",
    "`In summary`, feature scaling in KNN helps to improve the performance and accuracy of the algorithm by ensuring that all features contribute equally to the distance calculations, leading to better classification results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
